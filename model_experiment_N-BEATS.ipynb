{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_experiment_lightGBM.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alaki22/Contacts/blob/main/model_experiment_N-BEATS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading Kaggle data sets directly into Colab**"
      ],
      "metadata": {
        "id": "SyZxTF7lf7jk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the kaggle python library"
      ],
      "metadata": {
        "id": "7lvdgeEMgCoy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlwSaX9akGfG",
        "outputId": "1af396f2-c0eb-4cf4-bfc2-a4a21cbbc033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount the Google drive so you can store your kaggle API credentials for future use"
      ],
      "metadata": {
        "id": "rw0DfSAggHED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGineQt7dErh",
        "outputId": "21ce1401-25e9-4d75-eef8-4006195e2e66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a directory for kaggle at the temporary instance location on Colab drive.\n",
        "\n",
        "Download your kaggle API key (.json file). You can do this by going to your kaggle account page and clicking 'Create new API token' under the API section."
      ],
      "metadata": {
        "id": "Rvmi3WbigOmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "ZTkKggcylXfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the json file to Google Drive and then copy to the temporary location."
      ],
      "metadata": {
        "id": "p3N4it0xrFmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "IQq6ZMyTrEfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the file permissions to read/write to the owner only"
      ],
      "metadata": {
        "id": "p3dHJgtLehrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "7ncAtrq2lg5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Competitions and Datasets are the two types of Kaggle data**"
      ],
      "metadata": {
        "id": "Rb3Zm9VMlu3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Download competition data**\n",
        "\n",
        "If you get 403 Forbidden error, you need to click 'Late Submission' on the Kaggle page for that competition."
      ],
      "metadata": {
        "id": "OrdSFfGjl3Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download -c walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0yNdtoRln8A",
        "outputId": "58e145d7-d4f1-466f-8960-2d59a47c7c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 417MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip, in case the downloaded file is zipped. Refresh the files on the left hand side to update the view."
      ],
      "metadata": {
        "id": "fRmXZnHghNAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAs9oVnNoziL",
        "outputId": "78258c2e-a6ae-478b-960e-7e7a4ca025cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
            "  inflating: features.csv.zip        \n",
            "  inflating: sampleSubmission.csv.zip  \n",
            "  inflating: stores.csv              \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: train.csv.zip           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gc # For garbage collection\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.expand_frame_repr', False)"
      ],
      "metadata": {
        "id": "RAb9vK9B7YFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stores = pd.read_csv('stores.csv')\n",
        "train = pd.read_csv(\"train.csv.zip\")\n",
        "features = pd.read_csv('features.csv.zip')\n",
        "sample = pd.read_csv('sampleSubmission.csv.zip')\n",
        "test = pd.read_csv('test.csv.zip')"
      ],
      "metadata": {
        "id": "255em5G65SWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Date' columns to datetime objects for easier manipulation\n",
        "train['Date'] = pd.to_datetime(train['Date'])\n",
        "test['Date'] = pd.to_datetime(test['Date'])\n",
        "features['Date'] = pd.to_datetime(features['Date'])\n",
        "\n",
        "# Merge features with train and test data.\n",
        "# Note: 'IsHoliday' is present in both train/test and features.csv.\n",
        "# We'll merge on it to ensure consistency, but if there were discrepancies,\n",
        "# we'd need a more careful merge strategy.\n",
        "train_df = pd.merge(train, features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "test_df = pd.merge(test, features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "\n",
        "# Merge store information\n",
        "train_df = pd.merge(train_df, stores, on='Store', how='left')\n",
        "test_df = pd.merge(test_df, stores, on='Store', how='left')\n",
        "\n",
        "print(\"\\n--- Merged Train Data Head ---\")\n",
        "print(train_df.head())\n",
        "print(\"\\n--- Merged Test Data Head ---\")\n",
        "print(test_df.head())\n",
        "\n",
        "print(\"\\n--- Merged Train Data Info ---\")\n",
        "print(train_df.info())\n",
        "print(\"\\n--- Merged Test Data Info ---\")\n",
        "print(test_df.info())\n",
        "\n",
        "# Free up memory\n",
        "del train, test, features, stores\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "OFPJWG6V5nZ3",
        "outputId": "353cb388-cc06-492d-a99a-55b982319fe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Merged Train Data Head ---\n",
            "   Store  Dept       Date  Weekly_Sales  IsHoliday  Temperature  Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment Type    Size\n",
            "0      1     1 2010-02-05      24924.50      False        42.31       2.572        NaN        NaN        NaN        NaN        NaN  211.096358         8.106    A  151315\n",
            "1      1     1 2010-02-12      46039.49       True        38.51       2.548        NaN        NaN        NaN        NaN        NaN  211.242170         8.106    A  151315\n",
            "2      1     1 2010-02-19      41595.55      False        39.93       2.514        NaN        NaN        NaN        NaN        NaN  211.289143         8.106    A  151315\n",
            "3      1     1 2010-02-26      19403.54      False        46.63       2.561        NaN        NaN        NaN        NaN        NaN  211.319643         8.106    A  151315\n",
            "4      1     1 2010-03-05      21827.90      False        46.50       2.625        NaN        NaN        NaN        NaN        NaN  211.350143         8.106    A  151315\n",
            "\n",
            "--- Merged Test Data Head ---\n",
            "   Store  Dept       Date  IsHoliday  Temperature  Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment Type    Size\n",
            "0      1     1 2012-11-02      False        55.32       3.386    6766.44    5147.70      50.82    3639.90    2737.42  223.462779         6.573    A  151315\n",
            "1      1     1 2012-11-09      False        61.24       3.314   11421.32    3370.89      40.28    4646.79    6154.16  223.481307         6.573    A  151315\n",
            "2      1     1 2012-11-16      False        52.92       3.252    9696.28     292.10     103.78    1133.15    6612.69  223.512911         6.573    A  151315\n",
            "3      1     1 2012-11-23       True        56.23       3.211     883.59       4.17   74910.32     209.91     303.32  223.561947         6.573    A  151315\n",
            "4      1     1 2012-11-30      False        52.34       3.207    2460.03        NaN    3838.35     150.57    6966.34  223.610984         6.573    A  151315\n",
            "\n",
            "--- Merged Train Data Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 421570 entries, 0 to 421569\n",
            "Data columns (total 16 columns):\n",
            " #   Column        Non-Null Count   Dtype         \n",
            "---  ------        --------------   -----         \n",
            " 0   Store         421570 non-null  int64         \n",
            " 1   Dept          421570 non-null  int64         \n",
            " 2   Date          421570 non-null  datetime64[ns]\n",
            " 3   Weekly_Sales  421570 non-null  float64       \n",
            " 4   IsHoliday     421570 non-null  bool          \n",
            " 5   Temperature   421570 non-null  float64       \n",
            " 6   Fuel_Price    421570 non-null  float64       \n",
            " 7   MarkDown1     150681 non-null  float64       \n",
            " 8   MarkDown2     111248 non-null  float64       \n",
            " 9   MarkDown3     137091 non-null  float64       \n",
            " 10  MarkDown4     134967 non-null  float64       \n",
            " 11  MarkDown5     151432 non-null  float64       \n",
            " 12  CPI           421570 non-null  float64       \n",
            " 13  Unemployment  421570 non-null  float64       \n",
            " 14  Type          421570 non-null  object        \n",
            " 15  Size          421570 non-null  int64         \n",
            "dtypes: bool(1), datetime64[ns](1), float64(10), int64(3), object(1)\n",
            "memory usage: 48.6+ MB\n",
            "None\n",
            "\n",
            "--- Merged Test Data Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 115064 entries, 0 to 115063\n",
            "Data columns (total 15 columns):\n",
            " #   Column        Non-Null Count   Dtype         \n",
            "---  ------        --------------   -----         \n",
            " 0   Store         115064 non-null  int64         \n",
            " 1   Dept          115064 non-null  int64         \n",
            " 2   Date          115064 non-null  datetime64[ns]\n",
            " 3   IsHoliday     115064 non-null  bool          \n",
            " 4   Temperature   115064 non-null  float64       \n",
            " 5   Fuel_Price    115064 non-null  float64       \n",
            " 6   MarkDown1     114915 non-null  float64       \n",
            " 7   MarkDown2     86437 non-null   float64       \n",
            " 8   MarkDown3     105235 non-null  float64       \n",
            " 9   MarkDown4     102176 non-null  float64       \n",
            " 10  MarkDown5     115064 non-null  float64       \n",
            " 11  CPI           76902 non-null   float64       \n",
            " 12  Unemployment  76902 non-null   float64       \n",
            " 13  Type          115064 non-null  object        \n",
            " 14  Size          115064 non-null  int64         \n",
            "dtypes: bool(1), datetime64[ns](1), float64(9), int64(3), object(1)\n",
            "memory usage: 12.4+ MB\n",
            "None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "xWF9hReAgwk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "n61eRLSjg9Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WalmartDataset(Dataset):\n",
        "    \"\"\"Custom dataset for Walmart sales data\"\"\"\n",
        "    def __init__(self, X, y, lookback_window=52):\n",
        "        self.X = torch.FloatTensor(X)\n",
        "        self.y = torch.FloatTensor(y)\n",
        "        self.lookback_window = lookback_window\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X) - self.lookback_window\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx:idx + self.lookback_window]\n",
        "        y = self.y[idx + self.lookback_window]\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "-fG1O6T8g92L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NBeatsBlock(nn.Module):\n",
        "    \"\"\"Single N-BEATS block\"\"\"\n",
        "    def __init__(self, input_size, theta_size, basis_function, layers, layer_size):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.theta_size = theta_size\n",
        "        self.basis_function = basis_function\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "        self.fc_layers.append(nn.Linear(input_size, layer_size))\n",
        "        for _ in range(layers - 1):\n",
        "            self.fc_layers.append(nn.Linear(layer_size, layer_size))\n",
        "\n",
        "        # Theta layer\n",
        "        self.theta_layer = nn.Linear(layer_size, theta_size)\n",
        "\n",
        "        # Basis functions\n",
        "        if basis_function == 'generic':\n",
        "            self.backcast_basis = nn.Linear(theta_size, input_size)\n",
        "            self.forecast_basis = nn.Linear(theta_size, 1)  # Forecasting 1 step ahead\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward through fully connected layers\n",
        "        for layer in self.fc_layers:\n",
        "            x = torch.relu(layer(x))\n",
        "\n",
        "        # Get theta\n",
        "        theta = self.theta_layer(x)\n",
        "\n",
        "        # Generate backcast and forecast\n",
        "        if self.basis_function == 'generic':\n",
        "            backcast = self.backcast_basis(theta)\n",
        "            forecast = self.forecast_basis(theta)\n",
        "\n",
        "        return backcast, forecast"
      ],
      "metadata": {
        "id": "Ab9q_qAkhCQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NBeatsNet(nn.Module):\n",
        "    \"\"\"N-BEATS neural network\"\"\"\n",
        "    def __init__(self, input_size, stacks=2, blocks_per_stack=3, layers=4, layer_size=256, theta_size=16):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.stacks = stacks\n",
        "        self.blocks_per_stack = blocks_per_stack\n",
        "\n",
        "        # Create blocks\n",
        "        self.blocks = nn.ModuleList()\n",
        "        for stack in range(stacks):\n",
        "            for block in range(blocks_per_stack):\n",
        "                self.blocks.append(\n",
        "                    NBeatsBlock(\n",
        "                        input_size=input_size,\n",
        "                        theta_size=theta_size,\n",
        "                        basis_function='generic',\n",
        "                        layers=layers,\n",
        "                        layer_size=layer_size\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the input for processing\n",
        "        batch_size = x.shape[0]\n",
        "        x = x.view(batch_size, -1)  # Flatten to (batch_size, input_size)\n",
        "\n",
        "        residual = x\n",
        "        forecast = torch.zeros(batch_size, 1, device=x.device)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            backcast, block_forecast = block(residual)\n",
        "            residual = residual - backcast\n",
        "            forecast = forecast + block_forecast\n",
        "\n",
        "        return forecast"
      ],
      "metadata": {
        "id": "T-rrbedLhLvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MissingValueImputer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Custom Transformer to handle missing values for specific columns.\n",
        "    - MarkDown columns: fill with 0.\n",
        "    - Other specified numerical columns: fill with ffill then bfill, fallback to mean.\n",
        "    \"\"\"\n",
        "    def __init__(self, markdown_cols=None, numerical_cols_to_impute=None):\n",
        "        self.markdown_cols = markdown_cols if markdown_cols is not None else [f'MarkDown{i}' for i in range(1, 6)]\n",
        "        self.numerical_cols_to_impute = numerical_cols_to_impute if numerical_cols_to_impute is not None else ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "        self.means = {} # To store means for fallback imputation during transform\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Calculate means for fallback imputation from the training data\n",
        "        for col in self.numerical_cols_to_impute:\n",
        "            if col in X.columns:\n",
        "                self.means[col] = X[col].mean()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "\n",
        "\n",
        "        for col in self.markdown_cols:\n",
        "          if col in X_copy.columns:\n",
        "            X_copy[f\"{col}_was_missing\"] = X_copy[col].isna().astype(int)\n",
        "            X_copy[col] = X_copy[col].fillna(0)\n",
        "\n",
        "\n",
        "        # Impute other numerical columns with ffill then bfill, fallback to mean\n",
        "        for col in self.numerical_cols_to_impute:\n",
        "            if col in X_copy.columns:\n",
        "                X_copy[col] = X_copy[col].fillna(method='ffill').fillna(method='bfill')\n",
        "                # Fallback to mean if NaNs still exist (e.g., if all values were NaN in a column)\n",
        "                if X_copy[col].isnull().any() and col in self.means:\n",
        "                    X_copy[col] = X_copy[col].fillna(self.means[col])\n",
        "        return X_copy"
      ],
      "metadata": {
        "id": "Mmtm1jALocJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NBEATSLabelEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Custom Transformer to encode categorical features.\n",
        "    N-BEATS works better with label-encoded categoricals than pandas categories.\n",
        "    \"\"\"\n",
        "    def __init__(self, categorical_cols=None):\n",
        "        self.categorical_cols = categorical_cols if categorical_cols is not None else ['Store', 'Dept', 'Type']\n",
        "        self.label_encoders = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        for col in self.categorical_cols:\n",
        "            if col in X.columns:\n",
        "                self.label_encoders[col] = LabelEncoder()\n",
        "                self.label_encoders[col].fit(X[col].astype(str))\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "        for col in self.categorical_cols:\n",
        "            if col in X_copy.columns and col in self.label_encoders:\n",
        "                # Handle unseen categories by using a default value\n",
        "                X_copy[col] = X_copy[col].astype(str)\n",
        "                known_categories = set(self.label_encoders[col].classes_)\n",
        "                X_copy[col] = X_copy[col].apply(lambda x: x if x in known_categories else 'unknown')\n",
        "\n",
        "                # Add 'unknown' to encoder if needed\n",
        "                if 'unknown' not in self.label_encoders[col].classes_:\n",
        "                    current_classes = list(self.label_encoders[col].classes_)\n",
        "                    current_classes.append('unknown')\n",
        "                    self.label_encoders[col].classes_ = np.array(current_classes)\n",
        "\n",
        "                X_copy[col] = self.label_encoders[col].transform(X_copy[col])\n",
        "        return X_copy"
      ],
      "metadata": {
        "id": "qLp89Z-PodN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessing_pipeline = Pipeline([\n",
        "    ('missing_value_imputer', MissingValueImputer()),\n",
        "    ('label_encoder', NBEATSLabelEncoder())\n",
        "])\n",
        "\n",
        "print(\"Preparing training data...\")\n",
        "X_train = train_df.drop(['Weekly_Sales'], axis=1)\n",
        "y_train = train_df['Weekly_Sales']\n",
        "\n",
        "print(\"\\n--- Applying Preprocessing Pipeline to Train Data ---\")\n",
        "X_train_processed = preprocessing_pipeline.fit_transform(X_train, y_train)\n",
        "\n",
        "print(\"\\n--- Applying Preprocessing Pipeline to Test Data ---\")\n",
        "# For the test set, we only call transform, as fit was done on the training data.\n",
        "X_test_processed = preprocessing_pipeline.transform(test_df.drop(columns=['Id'], errors='ignore'))\n",
        "\n",
        "\n",
        "print(\"\\nProcessed X_train_processed info:\")\n",
        "print(X_train_processed.info())\n",
        "print(\"\\nProcessed X_test_processed info:\")\n",
        "print(X_test_processed.info())\n",
        "\n",
        "# Verify no missing values in processed data\n",
        "print(\"\\nMissing values in processed X_train_processed:\\n\", X_train_processed.isnull().sum().sum())\n",
        "print(\"Missing values in processed X_test_processed:\\n\", X_test_processed.isnull().sum().sum())"
      ],
      "metadata": {
        "id": "YCf0xgfJIESZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "validation_cutoff_date = pd.to_datetime('2012-09-01')\n",
        "\n",
        "# Sort by date\n",
        "single_series_df = X_train_processed[(X_train_processed[\"Store\"] == 1) & (X_train_processed[\"Dept\"] == 1)].copy()\n",
        "single_series_df = single_series_df.sort_values(\"Date\")\n",
        "\n",
        "# Use the same cutoff\n",
        "train_series = single_series_df[single_series_df[\"Date\"] < validation_cutoff_date]\n",
        "val_series = single_series_df[single_series_df[\"Date\"] >= validation_cutoff_date]\n",
        "\n",
        "# Normalize based on training only\n",
        "sales_mean = train_series[\"Weekly_Sales\"].mean()\n",
        "sales_std = train_series[\"Weekly_Sales\"].std()\n",
        "\n",
        "train_sales = (train_series[\"Weekly_Sales\"].values - sales_mean) / sales_std\n",
        "val_sales = (val_series[\"Weekly_Sales\"].values - sales_mean) / sales_std  # normalize using train stats\n",
        "\n",
        "# Prepare dataset\n",
        "lookback_window = 52\n",
        "X_train = train_sales.reshape(-1, 1)\n",
        "y_train = train_sales.reshape(-1)\n",
        "\n",
        "X_val = val_sales.reshape(-1, 1)\n",
        "y_val = val_sales.reshape(-1)\n",
        "\n",
        "train_dataset = WalmartDataset(X_train, y_train, lookback_window)\n",
        "val_dataset = WalmartDataset(X_val, y_val, lookback_window)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "def weighted_mean_absolute_error(y_true, y_pred, weights):\n",
        "    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "val_weights = np.where(val_series['IsHoliday'] == 1, 5, 1)\n",
        "train_weights_split = np.where(train_series['IsHoliday'] == 1, 5, 1)"
      ],
      "metadata": {
        "id": "U1ow-8b8xLm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zHUsa_yGFBIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DATA CLEANING**\n"
      ],
      "metadata": {
        "id": "XSajWBEMo4CA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q dagshub\n"
      ],
      "metadata": {
        "id": "nRj0MDGdJcdV",
        "outputId": "1eac0a4c-90c9-420c-aec2-7cbe149729e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/261.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow==2.7.1"
      ],
      "metadata": {
        "id": "Slx50n63bsZY",
        "outputId": "7ea44a6e-51d3-4773-b641-9b80eb94b4e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mlflow==2.7.1\n",
            "  Downloading mlflow-2.7.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (8.2.1)\n",
            "Collecting cloudpickle<3 (from mlflow==2.7.1)\n",
            "  Downloading cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting databricks-cli<1,>=0.8.7 (from mlflow==2.7.1)\n",
            "  Downloading databricks_cli-0.18.0-py2.py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (0.4)\n",
            "Requirement already satisfied: gitpython<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (3.1.44)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (6.0.2)\n",
            "Collecting protobuf<5,>=3.12.0 (from mlflow==2.7.1)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting pytz<2024 (from mlflow==2.7.1)\n",
            "  Downloading pytz-2023.4-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (2.32.3)\n",
            "Collecting packaging<24 (from mlflow==2.7.1)\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting importlib-metadata!=4.7.0,<7,>=3.7.0 (from mlflow==2.7.1)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (0.5.3)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow==2.7.1)\n",
            "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting docker<7,>=4.0.0 (from mlflow==2.7.1)\n",
            "  Downloading docker-6.1.3-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting Flask<3 (from mlflow==2.7.1)\n",
            "  Downloading flask-2.3.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting numpy<2 (from mlflow==2.7.1)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (1.15.3)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (2.2.2)\n",
            "Collecting querystring-parser<2 (from mlflow==2.7.1)\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl.metadata (559 bytes)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (2.0.41)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (1.6.1)\n",
            "Collecting pyarrow<14,>=4.0.0 (from mlflow==2.7.1)\n",
            "  Downloading pyarrow-13.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (3.8.2)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (3.10.0)\n",
            "Collecting gunicorn<22 (from mlflow==2.7.1)\n",
            "  Downloading gunicorn-21.2.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (3.1.6)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow==2.7.1) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic!=1.10.0,<2->mlflow==2.7.1) (4.14.0)\n",
            "Requirement already satisfied: pyjwt>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.7.1) (2.10.1)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.7.1) (3.3.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.7.1) (0.9.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.7.1) (1.17.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.7 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.7.1) (2.4.0)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from docker<7,>=4.0.0->mlflow==2.7.1) (1.8.0)\n",
            "Requirement already satisfied: Werkzeug>=2.3.7 in /usr/local/lib/python3.11/dist-packages (from Flask<3->mlflow==2.7.1) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3->mlflow==2.7.1) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3->mlflow==2.7.1) (1.9.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=2.1.0->mlflow==2.7.1) (4.0.12)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow==2.7.1) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2<4,>=2.11->mlflow==2.7.1) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow==2.7.1) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow==2.7.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow==2.7.1) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow==2.7.1) (2025.6.15)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow==2.7.1) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow==2.7.1) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.7.1) (3.2.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow==2.7.1) (5.0.2)\n",
            "Downloading mlflow-2.7.1-py3-none-any.whl (18.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.5/18.5 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
            "Downloading databricks_cli-0.18.0-py2.py3-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-6.1.3-py3-none-any.whl (148 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask-2.3.3-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.1/96.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-21.2.0-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.2/80.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-13.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (40.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2023.4-py2.py3-none-any.whl (506 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.5/506.5 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: pytz, querystring-parser, protobuf, packaging, numpy, importlib-metadata, cloudpickle, pyarrow, gunicorn, Flask, docker, databricks-cli, alembic, mlflow\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.7.0\n",
            "    Uninstalling importlib_metadata-8.7.0:\n",
            "      Successfully uninstalled importlib_metadata-8.7.0\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 3.1.1\n",
            "    Uninstalling cloudpickle-3.1.1:\n",
            "      Successfully uninstalled cloudpickle-3.1.1\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: Flask\n",
            "    Found existing installation: Flask 3.1.1\n",
            "    Uninstalling Flask-3.1.1:\n",
            "      Successfully uninstalled Flask-3.1.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "pylibcudf-cu12 25.2.1 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 13.0.0 which is incompatible.\n",
            "distributed 2024.12.1 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "dask-expr 1.1.21 requires pyarrow>=14.0.1, but you have pyarrow 13.0.0 which is incompatible.\n",
            "bigframes 2.8.0 requires pyarrow>=15.0.2, but you have pyarrow 13.0.0 which is incompatible.\n",
            "dask 2024.12.1 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\n",
            "google-cloud-bigquery 3.34.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "db-dtypes 1.4.3 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 13.0.0 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Flask-2.3.3 alembic-1.16.2 cloudpickle-2.2.1 databricks-cli-0.18.0 docker-6.1.3 gunicorn-21.2.0 importlib-metadata-6.11.0 mlflow-2.7.1 numpy-1.26.4 packaging-23.2 protobuf-4.25.8 pyarrow-13.0.0 pytz-2023.4 querystring-parser-1.2.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "importlib_metadata",
                  "numpy",
                  "packaging"
                ]
              },
              "id": "6258f188f6344a58983975bf533d39ed"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import dagshub\n",
        "# Try to get credentials from environment first\n",
        "dagshub.init(\n",
        "    repo_owner='abarb22',\n",
        "    repo_name='Walmart-Recruiting---Store-Sales-Forecasting',\n",
        "    mlflow=True\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "26gLz9xCSEt7",
        "outputId": "8a6cc377-02cd-438d-f242-977bd3c02b5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"abarb22/Walmart-Recruiting---Store-Sales-Forecasting\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"abarb22/Walmart-Recruiting---Store-Sales-Forecasting\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository abarb22/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository abarb22/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "class ModelTrainer:\n",
        "    def __init__(self, model, train_loader, val_loader, sales_mean, sales_std, val_weights, lr=1e-3, num_epochs=100):\n",
        "        self.model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.sales_mean = sales_mean\n",
        "        self.sales_std = sales_std\n",
        "        self.val_weights = val_weights\n",
        "        self.lr = lr\n",
        "        self.num_epochs = num_epochs\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.val_preds = []\n",
        "        self.val_targets = []\n",
        "\n",
        "    def train(self):\n",
        "        for epoch in range(self.num_epochs):\n",
        "            self.model.train()\n",
        "            running_train_loss = 0.0\n",
        "\n",
        "            for x_batch, y_batch in self.train_loader:\n",
        "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "                self.optimizer.zero_grad()\n",
        "                output = self.model(x_batch)\n",
        "                loss = self.criterion(output.squeeze(), y_batch)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                running_train_loss += loss.item() * x_batch.size(0)\n",
        "\n",
        "            train_loss = running_train_loss / len(self.train_loader.dataset)\n",
        "            self.train_losses.append(train_loss)\n",
        "\n",
        "            # Validation\n",
        "            self.model.eval()\n",
        "            running_val_loss = 0.0\n",
        "            self.val_preds.clear()\n",
        "            self.val_targets.clear()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for x_batch, y_batch in self.val_loader:\n",
        "                    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "                    output = self.model(x_batch)\n",
        "                    loss = self.criterion(output.squeeze(), y_batch)\n",
        "                    running_val_loss += loss.item() * x_batch.size(0)\n",
        "                    self.val_preds.extend(output.squeeze().cpu().numpy())\n",
        "                    self.val_targets.extend(y_batch.cpu().numpy())\n",
        "\n",
        "            val_loss = running_val_loss / len(self.val_loader.dataset)\n",
        "            self.val_losses.append(val_loss)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{self.num_epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def evaluate(self):\n",
        "        denorm_preds = np.array(self.val_preds) * self.sales_std + self.sales_mean\n",
        "        denorm_targets = np.array(self.val_targets) * self.sales_std + self.sales_mean\n",
        "        wmae = weighted_mean_absolute_error(denorm_targets, denorm_preds, self.val_weights[lookback_window:])\n",
        "        return wmae\n",
        "\n",
        "    def plot_losses(self, save_path=None):\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(self.train_losses, label='Train Loss')\n",
        "        plt.plot(self.val_losses, label='Val Loss')\n",
        "        plt.legend()\n",
        "        plt.title(\"Loss Curves\")\n",
        "        plt.grid(True)\n",
        "        if save_path:\n",
        "            plt.savefig(save_path)\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "HGeBbvQhFKHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store, dept = 1, 1  # just one combo to overfit\n",
        "\n",
        "df = train_data[(train_data['Store'] == store) & (train_data['Dept'] == dept)].copy()\n",
        "df = df.sort_values('Date')\n",
        "\n",
        "sales = df['Weekly_Sales'].values.astype(np.float32)\n",
        "weights = df['IsHoliday'].apply(lambda x: 5 if x else 1).values.astype(np.float32)\n",
        "\n",
        "sales_mean = sales.mean()\n",
        "sales_std = sales.std() if sales.std() > 0 else 1\n",
        "sales_norm = (sales - sales_mean) / sales_std\n",
        "\n",
        "X, y = create_sequences(sales_norm, lookback_window)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "model = NBeatsNet(input_size=lookback_window)\n",
        "\n",
        "trainer = ModelTrainer(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    sales_mean=sales_mean,\n",
        "    sales_std=sales_std,\n",
        "    val_weights=weights,\n",
        "    lr=1e-3,\n",
        "    num_epochs=100\n",
        ")\n"
      ],
      "metadata": {
        "id": "y2X9rkP2FRVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with mlflow.start_run(run_name=f\"Overfit_Store_{store}_Dept_{dept}\"):\n",
        "    trainer.train()\n",
        "    wmae = trainer.evaluate()\n",
        "\n",
        "    # Log params and metrics\n",
        "    mlflow.log_param(\"store\", store)\n",
        "    mlflow.log_param(\"dept\", dept)\n",
        "    mlflow.log_param(\"lookback_window\", lookback_window)\n",
        "    mlflow.log_param(\"epochs\", trainer.num_epochs)\n",
        "    mlflow.log_param(\"batch_size\", 32)\n",
        "    mlflow.log_metric(\"val_wmae\", wmae)\n",
        "    mlflow.log_metric(\"val_loss\", trainer.val_losses[-1])\n",
        "\n",
        "    # Log loss plot\n",
        "    plot_path = f\"loss_plot_store{store}_dept{dept}.png\"\n",
        "    trainer.plot_losses(save_path=plot_path)\n",
        "    mlflow.log_artifact(plot_path)\n",
        "\n",
        "    # Log trained model\n",
        "    mlflow.pytorch.log_model(trainer.model, artifact_path=\"model\")"
      ],
      "metadata": {
        "id": "yPe6YCkGFmWW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}